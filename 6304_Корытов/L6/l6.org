#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+PROPERTY: header-args:python :session *l6*
#+PROPERTY: header-args:python+ :exports both
#+PROPERTY: header-args:python+ :tangle yes
#+PROPERTY: header-args:python+ :async yes

#+begin_src elisp :exports none
(setq-local org-image-actual-width '(1024))
(setq-local org-html-htmlize-output-type 'css)
(setq-local org-latex-listings 'minted)
#+end_src

#+RESULTS:
: minted

* Загрузка данных
#+begin_src python :display plain
import pandas as pd
import numpy as np
from IPython.display import display

df = pd.read_csv('../data/CC_GENERAL.csv').iloc[:,1:].dropna()

with open('./output/data.txt', 'w') as f:
    f.write(str(df))

df
#+end_src

#+RESULTS:
#+begin_example
            BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \
  0       40.900749           0.818182      95.40              0.00   
  1     3202.467416           0.909091       0.00              0.00   
  2     2495.148862           1.000000     773.17            773.17   
  4      817.714335           1.000000      16.00             16.00   
  5     1809.828751           1.000000    1333.28              0.00   
  ...           ...                ...        ...               ...   
  8943     5.871712           0.500000      20.90             20.90   
  8945    28.493517           1.000000     291.12              0.00   
  8947    23.398673           0.833333     144.40              0.00   
  8948    13.457564           0.833333       0.00              0.00   
  8949   372.708075           0.666667    1093.25           1093.25   

        INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \
  0                      95.40      0.000000             0.166667   
  1                       0.00   6442.945483             0.000000   
  2                       0.00      0.000000             1.000000   
  4                       0.00      0.000000             0.083333   
  5                    1333.28      0.000000             0.666667   
  ...                      ...           ...                  ...   
  8943                    0.00      0.000000             0.166667   
  8945                  291.12      0.000000             1.000000   
  8947                  144.40      0.000000             0.833333   
  8948                    0.00     36.558778             0.000000   
  8949                    0.00    127.040008             0.666667   

        ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \
  0                       0.000000                          0.083333   
  1                       0.000000                          0.000000   
  2                       1.000000                          0.000000   
  4                       0.083333                          0.000000   
  5                       0.000000                          0.583333   
  ...                          ...                               ...   
  8943                    0.166667                          0.000000   
  8945                    0.000000                          0.833333   
  8947                    0.000000                          0.666667   
  8948                    0.000000                          0.000000   
  8949                    0.666667                          0.000000   

        CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \
  0                   0.000000                 0              2        1000.0   
  1                   0.250000                 4              0        7000.0   
  2                   0.000000                 0             12        7500.0   
  4                   0.000000                 0              1        1200.0   
  5                   0.000000                 0              8        1800.0   
  ...                      ...               ...            ...           ...   
  8943                0.000000                 0              1         500.0   
  8945                0.000000                 0              6        1000.0   
  8947                0.000000                 0              5        1000.0   
  8948                0.166667                 2              0         500.0   
  8949                0.333333                 2             23        1200.0   

           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  
  0      201.802084        139.509787          0.000000      12  
  1     4103.032597       1072.340217          0.222222      12  
  2      622.066742        627.284787          0.000000      12  
  4      678.334763        244.791237          0.000000      12  
  5     1400.057770       2407.246035          0.000000      12  
  ...           ...               ...               ...     ...  
  8943    58.644883         43.473717          0.000000       6  
  8945   325.594462         48.886365          0.500000       6  
  8947    81.270775         82.418369          0.250000       6  
  8948    52.549959         55.755628          0.250000       6  
  8949    63.165404         88.288956          0.000000       6  

  [8636 rows x 17 columns]
#+end_example

** Стандартизация
#+begin_src python :display plain :async yes
from sklearn import preprocessing

columns = df.columns
data = np.array(df, dtype='float')
scaler = preprocessing.StandardScaler()
scaled_data = scaler.fit_transform(data)
#+end_src

#+RESULTS:

** Проверка всякая

#+begin_src python :async yes
from matplotlib import pyplot as plt
import matplotlib as mpl

mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
#+end_src

#+RESULTS:

#+begin_src python :file img/hist.png
def make_hists(data):
    fig, axes = plt.subplots(4, 5, figsize=(16, 16))
    axes = [a for ax1 in axes for a in ax1]
    for i in range(data.shape[1]):
        row = data[:,i]
        axes[i].hist(row)
        axes[i].set_title(columns[i][:20])
    
make_hists(scaled_data)
#+end_src

#+RESULTS:
[[file:img/hist.png]]

* DBSCAN
** DBSCAN без параметров
#+begin_src python
from sklearn.cluster import DBSCAN

clustering = DBSCAN().fit(scaled_data)

print(set(clustering.labels_))
print(len(set(clustering.labels_)) - 1)
print(list(clustering.labels_).count(-1) /  len(list(clustering.labels_)))

with open('./output/dbscan.txt', 'w') as f:
    f.write(f'Метки кластеров: {set(clustering.labels_)}\n')
    f.write(f'Количество кластеров: {len(set(clustering.labels_)) - 1}\n')
    f.write(f'Процент выпавших: {list(clustering.labels_).count(-1) /  len(list(clustering.labels_))}\n')
#+end_src

#+RESULTS:
: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, -1}
: 36
: 0.7512737378415933
** Процент выпавших и дистанция
#+begin_src python
eps_v = np.linspace(0.1, 2, 15)

def get_outliers(dbscan):
    return list(dbscan.labels_).count(-1) /  len(list(dbscan.labels_))

def get_set_number(dbscan):
    return len(set(dbscan.labels_)) - 1
    
def get_largest_cluster(dbscan):
    clusters = {}
    for label in dbscan.labels_:
        if label == -1:
            continue
        try:
            clusters[label] += 1
        except KeyError:
            clusters[label] = 1
    if len(clusters) == 0:
        return 0
    return max(clusters.values()) / len(dbscan.labels_)
    
p_v, n_v, c_v = [], [], []

for eps in eps_v:
    dbscan = DBSCAN(eps=eps).fit(scaled_data)
    p_v.append(get_outliers(dbscan))
    n_v.append(get_set_number(dbscan))
    c_v.append(get_largest_cluster(dbscan))

p_v
#+end_src

#+RESULTS:
| 0.9845993515516442 | 0.9341130152848541 | 0.8593098656785549 | 0.740968040759611 | 0.6222788327929597 | 0.5182955071792497 | 0.4172070402964335 | 0.3213293191292265 | 0.2562528948587309 | 0.1995136637332098 | 0.16257526632700325 | 0.13131079203334878 | 0.10421491431218156 | 0.08499305233904586 | 0.06866604909680407 |

#+begin_src python :file img/p_v.png
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

axes[0].plot(eps_v, p_v)
axes[0].set_xlabel('eps')
axes[0].set_ylabel('Процент выпавших')

axes[1].plot(eps_v, n_v)
axes[1].set_xlabel('eps')
axes[1].set_ylabel('Количество кластеров')

axes[2].plot(eps_v, c_v)
axes[2].set_xlabel('eps')
axes[2].set_ylabel('Самый большой кластер')

[ax.set_ylim(0) for ax in axes]
[ax.grid(0.75) for ax in axes]

fig.tight_layout()
#+end_src

#+RESULTS:
[[file:img/p_v.png]]
** Процент выпавших и количество точек в кластере
#+begin_src python
min_samples_v = [*range(2, 15, 1)]
p_v2, n_v2, c_v2 = [], [], []
for min_samples in min_samples_v:
    dbscan = DBSCAN(min_samples=min_samples).fit(scaled_data)
    p_v2.append(get_outliers(dbscan))
    n_v2.append(get_set_number(dbscan))
    c_v2.append(get_largest_cluster(dbscan))

p_v2
#+end_src

#+RESULTS:
| 0.640226956924502 | 0.6883974062065771 | 0.7267253358036128 | 0.7512737378415933 | 0.7702640111162575 | 0.7836961556276054 | 0.796317739694303 | 0.8033811949976841 | 0.8141500694766095 | 0.822024085224641 | 0.8260768874478925 | 0.8290875405280222 | 0.8332561371005095 |

#+begin_src python :file img/p_v2.png
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

axes[0].plot(min_samples_v, p_v2)
axes[0].grid(0.75)
axes[0].set_xlabel('min_samples')
axes[0].set_ylabel('Процент выпавших')
axes[0].set_ylim(0)

axes[1].plot(min_samples_v, n_v2)
axes[1].grid(0.75)
axes[1].set_xlabel('min_samples')
axes[1].set_ylabel('Количество кластеров')
axes[1].set_ylim(0)

axes[2].plot(min_samples_v, c_v2)
axes[2].grid(0.75)
axes[2].set_xlabel('min_samples')
axes[2].set_ylabel('Самый большой кластер')
axes[2].set_ylim(0)

fig.tight_layout()
#+end_src

#+RESULTS:
[[file:img/p_v2.png]]
** Значения параметров
#+begin_src python
p_matrix = []
n_matrix = []
c_matrix = []

for min_samples in min_samples_v:
    p_matrix_row = []
    n_matrix_row = []
    c_matrix_row = []
    for eps in eps_v:
        dbscan = DBSCAN(min_samples=min_samples, eps=eps).fit(scaled_data)
        p = get_outliers(dbscan)
        n = get_set_number(dbscan)
        c = get_largest_cluster(dbscan)
        
        p_matrix_row.append(p)
        n_matrix_row.append(n)
        c_matrix_row.append(c)
    p_matrix.append(p_matrix_row)
    n_matrix.append(n_matrix_row)
    c_matrix.append(c_matrix_row)
#+end_src

#+RESULTS:

#+begin_src python :file img/mat.png
p_matrix_n = np.array(p_matrix)
n_matrix_n = np.array(n_matrix)
c_matrix_n = np.array(c_matrix)

fig, axes = plt.subplots(1, 3, figsize=(10, 10))

axes[0].matshow(np.vectorize(lambda v: v * 100 if v <= 0.12 else 0)(p_matrix_n))
axes[1].matshow(np.vectorize(lambda v: v if 5 <= v <= 7 else 0)(n_matrix_n))
axes[2].matshow(np.vectorize(lambda v: max(0.5 - v, 0))(c_matrix_n))

for ax in axes:
    ax.set_xticks(range(len(eps_v)))
    ax.set_xticklabels([f'{v:.2f}' for v in eps_v], rotation=90)
    ax.set_yticks(range(len(min_samples_v)))
    ax.set_yticklabels(min_samples_v)
    ax.set_xlabel('eps')
    ax.set_ylabel('min_samples')

axes[0].set_title('Процент выпавших', pad=25)
axes[1].set_title('Количество кластеров', pad=25)
axes[2].set_title('Самый большой кластер', pad=25)

for i, min_samples in enumerate(min_samples_v):
    for j, eps in enumerate(eps_v):
        p = p_matrix_n[i, j]
        n = n_matrix_n[i, j]
        c = c_matrix_n[i, j]
        axes[0].text(j, i, f'{p:.2f}', ha='center', va='center', size=5)
        axes[1].text(j, i, f'{n:.0f}', ha='center', va='center', size=5)
        axes[2].text(j, i, f'{c:.2f}', ha='center', va='center', size=5)
fig.tight_layout()
#+end_src

#+RESULTS:
[[file:img/mat.png]]
** Визуализация с понижением размерности
#+begin_src python :file img/pca.png
from sklearn.decomposition import PCA
min_samples = 3
eps = 2

db = DBSCAN(min_samples=min_samples, eps=eps).fit(scaled_data)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
          
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(scaled_data)
unique_labels.remove(-1)
unique_labels = [-1, *list(unique_labels)]
          
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = reduced_data[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6, alpha=1)
             
    xy = reduced_data[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14, alpha=1)
             
plt.title('Estimated number of clusters: %d' % n_clusters_)
pass
#+end_src

#+RESULTS:
[[file:img/pca.png]]
* OPTICS
** Параметры
#+begin_src python
dbscan = DBSCAN(min_samples=min_samples, eps=eps).fit(scaled_data)

print(len(set(dbscan.labels_)) - 1)
print(list(dbscan.labels_).count(-1) /  len(list(dbscan.labels_)))

with open('./output/dbscan_good.txt', 'w') as f:
    f.write(f'Количество кластеров: {len(set(dbscan.labels_)) - 1}\n')
    f.write(f'Процент выпавших: {list(dbscan.labels_).count(-1) /  len(list(dbscan.labels_))}\n')

#+end_src

#+RESULTS:
: 6
: 0.06287633163501621

#+begin_src python
optics_min_samples_v = []
optics_max_eps_v = []
optics_n_v = []
optics_p_v = []
optics_c_v = []
#+end_src

#+RESULTS:

#+begin_src python :file img/reachability.png
from sklearn.cluster import OPTICS
from matplotlib import cm

np.random.seed(42)

def random_color():
    cmap = cm.get_cmap('hsv')
    return cmap(np.random.random())

optics_min_samples = 15
optics_max_eps = np.inf

optics = OPTICS(min_samples=optics_min_samples, max_eps=optics_max_eps).fit(scaled_data)

optics_n = len(set(optics.labels_)) - 1
optics_p = list(optics.labels_).count(-1) /  len(list(optics.labels_))
optics_c = get_largest_cluster(optics)

print(optics_n, optics_p, optics_c)
optics_n_v.append(optics_n)
optics_p_v.append(optics_p)
optics_c_v.append(optics_c)
optics_min_samples_v.append(optics_min_samples)
optics_max_eps_v.append(optics_max_eps)
#+end_src

#+RESULTS:
: 11 0.9656090782769801 0.00810560444650301

#+begin_src python :display plain
import tabulate
df = pd.DataFrame({
    "min_samples": optics_min_samples_v,
    "max_eps": optics_max_eps_v,
    "Кластеров": optics_n_v,
    "Выпало": optics_p_v,
    "Самый большой": optics_c_v
})
with open('./output/optics.tex', 'w') as f:
    f.write(tabulate.tabulate(df, headers=df.columns, tablefmt='latex_booktabs'))
df
#+end_src

#+RESULTS:
:    min_samples  max_eps  Кластеров    Выпало  Самый большой
: 0            5      inf        112  0.898448       0.002663
: 1            4      inf        229  0.847499       0.002432
: 2            3      inf        524  0.733673       0.001853
: 3            2      inf       1629  0.506369       0.000926
: 4            2      1.0       1417  0.569940       0.000926
: 5            2      0.5        877  0.735410       0.000926
: 6            5      1.0        107  0.901575       0.002663
: 7            5      0.5         76  0.926934       0.002779
: 8           15      inf         11  0.965609       0.008106

#+begin_src python
optics = OPTICS(min_samples=min_samples, max_eps=eps, cluster_method='dbscan').fit(scaled_data)
optics_n = len(set(optics.labels_)) - 1
optics_p = list(optics.labels_).count(-1) /  len(list(optics.labels_))

print(optics_n, optics_p)
#+end_src

#+RESULTS:
: 6 0.06310792033348772

#+begin_src python :file img/reachability.png
space = np.arange(len(scaled_data))
reachability = optics.reachability_[optics.ordering_]
labels = optics.labels_[optics.ordering_]

colors = [random_color() for _ in set(labels)]
fig, ax = plt.subplots(figsize=(12, 6))

for class_, color in enumerate(colors):
    Xk = space[labels == class_]
    Rk = reachability[labels == class_]
    ax.plot(Xk, Rk, color, alpha=0.3, marker='.', lw=0, ms=10)
    
ax.plot(space[labels == -1], reachability[labels == -1], 'k', alpha=0.3, lw=0, ms=10, marker='.')
pass
#+end_src

#+RESULTS:
[[file:img/reachability.png]]
** Метрики
#+begin_src python
params_arr = []
results = []
#+end_src

#+RESULTS:

#+begin_src python
params = {'metric': 'sqeuclidean'}
optics = OPTICS(**params).fit(scaled_data)

result = {
    'params': params,
    'Кластеров': optics_n,
    'Выпавшие': optics_p,
    'Самый большой': optics_c
}
optics_n = len(set(optics.labels_)) - 1
optics_p = list(optics.labels_).count(-1) /  len(list(optics.labels_))
optics_c = get_largest_cluster(optics)
results.append(result)
print(result)
#+end_src

#+RESULTS:
: {'params': {'metric': 'sqeuclidean'}, 'Кластеров': 141, 'Выпавшие': 0.870194534506716, 'Самый большой': 0.0030106530801296896}

#+begin_src python :display plain
df = pd.DataFrame(results)
with open('./output/metrics.tex', 'w') as f:
    f.write(tabulate.tabulate(df, headers=df.columns, tablefmt='latex_booktabs'))
    
df
#+end_src

#+RESULTS:
#+begin_example
                                                 params  Кластеров  Выпавшие  \
  0                                    {'metric': 'l1'}         99  0.909217   
  1                  {'metric': 'l1', 'min_samples': 2}         99  0.909217   
  2                                    {'metric': 'l2'}       1545  0.543191   
  3                  {'metric': 'l2', 'min_samples': 2}        112  0.901112   
  4                 {'metric': 'l2', 'min_samples': 10}       1629  0.506369   
  5                 {'metric': 'l2', 'min_samples': 40}         23  0.952987   
  6                      {'metric': 'l2', 'max_eps': 1}          2  0.984831   
  7                             {'metric': 'manhattan'}        108  0.903659   
  8           {'metric': 'manhattan', 'min_samples': 2}         99  0.909217   
  9               {'metric': 'manhattan', 'max_eps': 1}       1545  0.543191   
  10  {'metric': 'manhattan', 'max_eps': 1, 'min_sam...         68  0.934808   
  11                            {'metric': 'chebyshev'}         17  0.967114   
  12          {'metric': 'chebyshev', 'min_samples': 2}        141  0.870195   
  13         {'metric': 'chebyshev', 'min_samples': 10}       1550  0.527559   
  14  {'metric': 'chebyshev', 'min_samples': 10, 'ma...         28  0.914775   
  15                            {'metric': 'chebyshev'}         28  0.914775   
  16                          {'metric': 'sqeuclidean'}        141  0.870195   

      Самый большой  
  0        0.002895  
  1        0.002895  
  2        0.000926  
  3        0.002663  
  4        0.000926  
  5        0.004516  
  6        0.009495  
  7        0.002663  
  8        0.002895  
  9        0.000926  
  10       0.002895  
  11       0.003126  
  12       0.003011  
  13       0.001390  
  14       0.007063  
  15       0.007063  
  16       0.003011  
#+end_example
